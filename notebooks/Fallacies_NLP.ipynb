{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfcc841-0186-4fb7-91e3-552d119ee718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeules and Scripts Imported Successfully\n",
      "Imports OK, labels: {'CHERRY_PICKING': 0, 'EVADING_THE_BURDEN_OF_PROOF': 1, 'FALSE_ANALOGY': 2, 'FALSE_AUTHORITY': 3, 'FALSE_CAUSE': 4, 'HASTY_GENERALISATION': 5, 'NO_FALLACY': 6, 'POST_HOC': 7, 'RED_HERRINGS': 8, 'STRAWMAN': 9, 'VAGUENESS': 10}\n"
     ]
    }
   ],
   "source": [
    "# Reading or making path/Dir available and accessible for conda to read\n",
    "# Import functions/modules from other python Scripts created\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.data.clean_text import basic_clean\n",
    "from src.models.baseline import BaselineConfig, run_baseline_multiclass\n",
    "from src.models.slm_finetune import SLMConfigMC, train_slm_multiclass\n",
    "from src.explainer_Qwen import QwenExplainer\n",
    "from src.labels_climate import ID2LABEL, LABEL2ID\n",
    "\n",
    "print(\"Modeules and Scripts Imported Successfully\")\n",
    "print(\"Imports OK, labels:\", LABEL2ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d6a0a7-3cad-4dfc-9711-47395462cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Importing and calling all required modules\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df38a1fd-fba3-43a5-99df-04dc0c462883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dir: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\raw\\climate\\train\n",
      "Dev dir: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\raw\\climate\\dev\n",
      "Test dir: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\raw\\climate\\test\n"
     ]
    }
   ],
   "source": [
    " # Raw CLIMATE data => just as in Tariq's work.\n",
    "RAW_CLIMATE_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"climate\"\n",
    "TRAIN_DIR = RAW_CLIMATE_DIR / \"train\"\n",
    "DEV_DIR   = RAW_CLIMATE_DIR / \"dev\"\n",
    "TEST_DIR  = RAW_CLIMATE_DIR / \"test\"\n",
    "\n",
    "# This is where we save cleaned CSVs\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Column name in the .tsv that holds the actual text\n",
    "TEXT_COL = \"fact_checked_segment\"\n",
    "\n",
    "print(\"Train dir:\", TRAIN_DIR)\n",
    "print(\"Dev dir:\", DEV_DIR)\n",
    "print(\"Test dir:\", TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d3022d-e9f7-40c3-97a1-fa6758d96699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_checked_segment</th>\n",
       "      <th>comment_by_fact-checker</th>\n",
       "      <th>article</th>\n",
       "      <th>label_str_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“climate economists see a positive externality...</td>\n",
       "      <td>This is cherry-picking at its worst. You can a...</td>\n",
       "      <td>article36.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest U.N. science compendium asserts tha...</td>\n",
       "      <td>The recent US National Climate Assessment1 fin...</td>\n",
       "      <td>article130.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“‘If we are right, our study challenges decade...</td>\n",
       "      <td>It only potentially challenges ONE method used...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cherry Picking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“a killer analysis conducted by Craig Idso of ...</td>\n",
       "      <td>This publication is not peer-reviewed, cherry-...</td>\n",
       "      <td>article45.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Next year or the year after that, I think it ...</td>\n",
       "      <td>I would also add that predictions of an ice-fr...</td>\n",
       "      <td>article33.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fact_checked_segment  \\\n",
       "0  “climate economists see a positive externality...   \n",
       "1  The latest U.N. science compendium asserts tha...   \n",
       "2  “‘If we are right, our study challenges decade...   \n",
       "3  “a killer analysis conducted by Craig Idso of ...   \n",
       "4  “Next year or the year after that, I think it ...   \n",
       "\n",
       "                             comment_by_fact-checker         article  \\\n",
       "0  This is cherry-picking at its worst. You can a...   article36.txt   \n",
       "1  The recent US National Climate Assessment1 fin...  article130.txt   \n",
       "2  It only potentially challenges ONE method used...             NaN   \n",
       "3  This publication is not peer-reviewed, cherry-...   article45.txt   \n",
       "4  I would also add that predictions of an ice-fr...   article33.txt   \n",
       "\n",
       "    label_str_raw  \n",
       "0  Cherry Picking  \n",
       "1  Cherry Picking  \n",
       "2  Cherry Picking  \n",
       "3  Cherry Picking  \n",
       "4  Cherry Picking  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_split(split_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Here we Read all .tsv files in a split directory, adds label from filename,\n",
    "    and concatenates into one DataFrame -> thus 1csv file.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for path in split_dir.glob(\"*.tsv\"):\n",
    "        label = path.stem  # filename without extension, e.g. \"Cherry Picking\"\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "        df[\"label_str_raw\"] = label\n",
    "        rows.append(df)\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "\n",
    "train_df_raw = load_split(TRAIN_DIR)\n",
    "dev_df_raw   = load_split(DEV_DIR)\n",
    "test_df_raw  = load_split(TEST_DIR)\n",
    "\n",
    "train_df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2076e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined CSVs to: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\n",
      "Saved:\n",
      " - C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\\climate_train_combined.csv\n",
      " - C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\\climate_dev_combined.csv\n",
      " - C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\\climate_test_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Putting all csv's  into a dir in data called combined_csv for reference\n",
    "# Just for backup purposes \n",
    "# creating the combined_csv folder\n",
    "COMBINED_DIR = PROJECT_ROOT / \"data\" / \"combined_csv\"\n",
    "COMBINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving combined CSVs to:\", COMBINED_DIR)\n",
    "\n",
    "# Here we go ahead and create the various csv files and save them to combined_csv\n",
    "train_comb_path = COMBINED_DIR / \"climate_train_combined.csv\"\n",
    "dev_comb_path   = COMBINED_DIR / \"climate_dev_combined.csv\"\n",
    "test_comb_path  = COMBINED_DIR / \"climate_test_combined.csv\"\n",
    "\n",
    "train_df_raw.to_csv(train_comb_path, index=False)\n",
    "dev_df_raw.to_csv(dev_comb_path, index=False)\n",
    "test_df_raw.to_csv(test_comb_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", train_comb_path)\n",
    "print(\" -\", dev_comb_path)\n",
    "print(\" -\", test_comb_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7063177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHERRY_PICKING',\n",
       " 'EVADING_THE_BURDEN_OF_PROOF',\n",
       " 'FALSE_ANALOGY',\n",
       " 'FALSE_AUTHORITY',\n",
       " 'FALSE_CAUSE',\n",
       " 'HASTY_GENERALIZATION',\n",
       " 'NO_FALLACY',\n",
       " 'POST_HOC',\n",
       " 'RED_HERRING',\n",
       " 'STRAWMAN',\n",
       " 'VAGUENESS']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalising Label Names\n",
    "# we create a function which makes changes to the label names in our file(csv) from lower case to UPPERCASE\n",
    "def normalize_label(label: str) -> str:\n",
    "    \"\"\"\n",
    "    e.g. \"Cherry Picking\" -> \"CHERRY_PICKING\"\n",
    "         \"No fallacy\"    -> \"NO_FALLACY\"\n",
    "    \"\"\"\n",
    "    return (\n",
    "        str(label)\n",
    "        .strip()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .upper()\n",
    "    )\n",
    "\n",
    "\n",
    "train_df_raw[\"label_str\"] = train_df_raw[\"label_str_raw\"].apply(normalize_label)\n",
    "dev_df_raw[\"label_str\"]   = dev_df_raw[\"label_str_raw\"].apply(normalize_label)\n",
    "test_df_raw[\"label_str\"]  = test_df_raw[\"label_str_raw\"].apply(normalize_label)\n",
    "\n",
    "sorted(train_df_raw[\"label_str\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d01267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing label_id in train: 0\n",
      "Missing label_id in dev:   0\n",
      "Missing label_id in test:  0\n"
     ]
    }
   ],
   "source": [
    "# Fix US/UK + singular/plural mismatches\n",
    "# Fix label variants to match labels_climate.py since some missing labels exists in data.\n",
    "label_corrections = {\n",
    "    \"HASTY_GENERALIZATION\": \"HASTY_GENERALISATION\",\n",
    "    \"RED_HERRING\": \"RED_HERRINGS\",\n",
    "}\n",
    "\n",
    "for df in [train_df_raw, dev_df_raw, test_df_raw]:\n",
    "    df[\"label_str\"] = df[\"label_str\"].replace(label_corrections)\n",
    "\n",
    "sorted(train_df_raw[\"label_str\"].unique())\n",
    "\n",
    "# Then we Map all labels to numeric IDs using LABEL2ID after adjusting label variants to match labels_climate.py\n",
    "train_df_raw[\"label_id\"] = train_df_raw[\"label_str\"].map(LABEL2ID)\n",
    "dev_df_raw[\"label_id\"]   = dev_df_raw[\"label_str\"].map(LABEL2ID)\n",
    "test_df_raw[\"label_id\"]  = test_df_raw[\"label_str\"].map(LABEL2ID)\n",
    "\n",
    "print(\"Missing label_id in train:\", train_df_raw[\"label_id\"].isna().sum())\n",
    "print(\"Missing label_id in dev:  \", dev_df_raw[\"label_id\"].isna().sum())\n",
    "print(\"Missing label_id in test: \", test_df_raw[\"label_id\"].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eccb1c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_checked_segment</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“climate economists see a positive externality...</td>\n",
       "      <td>climate economists see a positive externality,...</td>\n",
       "      <td>CHERRY_PICKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest U.N. science compendium asserts tha...</td>\n",
       "      <td>The latest U.N. science compendium asserts tha...</td>\n",
       "      <td>CHERRY_PICKING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fact_checked_segment  \\\n",
       "0  “climate economists see a positive externality...   \n",
       "1  The latest U.N. science compendium asserts tha...   \n",
       "\n",
       "                                          text_clean       label_str  \n",
       "0  climate economists see a positive externality,...  CHERRY_PICKING  \n",
       "1  The latest U.N. science compendium asserts tha...  CHERRY_PICKING  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we clean our text by calling or importing our function \"basic_clean\" from clean_text.py\n",
    "\n",
    "train_df_raw[\"text_clean\"] = train_df_raw[TEXT_COL].astype(str).apply(basic_clean)\n",
    "dev_df_raw[\"text_clean\"]   = dev_df_raw[TEXT_COL].astype(str).apply(basic_clean)\n",
    "test_df_raw[\"text_clean\"]  = test_df_raw[TEXT_COL].astype(str).apply(basic_clean)\n",
    "\n",
    "train_df_raw[[\"fact_checked_segment\", \"text_clean\", \"label_str\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c6d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: dropped 0 rows, kept 437\n",
      "dev: dropped 0 rows, kept 115\n",
      "test: dropped 0 rows, kept 133\n"
     ]
    }
   ],
   "source": [
    "#  Drop rows with missing text/labels, save processed CSVs ===\n",
    "\n",
    "for name, df in [(\"train\", train_df_raw), (\"dev\", dev_df_raw), (\"test\", test_df_raw)]:\n",
    "    before = df.shape[0]\n",
    "    df.dropna(subset=[\"text_clean\", \"label_id\"], inplace=True)\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\").astype(str)\n",
    "    after = df.shape[0]\n",
    "    print(f\"{name}: dropped {before - after} rows, kept {after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4d5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: dropped 0 rows with NaN text/label, kept 437\n",
      "dev: dropped 0 rows with NaN text/label, kept 115\n",
      "test: dropped 0 rows with NaN text/label, kept 133\n",
      "NaNs in train: 0 0\n",
      "NaNs in dev: 0 0\n",
      "NaNs in test: 0 0\n"
     ]
    }
   ],
   "source": [
    "# Removing and dropping rows with missing text or labels => NaN\n",
    "\n",
    "train_df_raw[\"text_clean\"].isna().sum(), train_df_raw[\"label_id\"].isna().sum()\n",
    "\n",
    "# Drop rows where text_clean or label_id is missing\n",
    "for name, df in [(\"train\", train_df_raw), (\"dev\", dev_df_raw), (\"test\", test_df_raw)]:\n",
    "    before = df.shape[0]\n",
    "    df.dropna(subset=[\"text_clean\", \"label_id\"], inplace=True)\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].astype(str)\n",
    "    after = df.shape[0]\n",
    "    print(f\"{name}: dropped {before - after} rows with NaN text/label, kept {after}\")\n",
    "\n",
    "# confirmation that there are no Nan in processed data \n",
    "print(\"NaNs in train:\", train_df_raw[\"text_clean\"].isna().sum(), train_df_raw[\"label_id\"].isna().sum())\n",
    "print(\"NaNs in dev:\", dev_df_raw[\"text_clean\"].isna().sum(), dev_df_raw[\"label_id\"].isna().sum())\n",
    "print(\"NaNs in test:\", test_df_raw[\"text_clean\"].isna().sum(), test_df_raw[\"label_id\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d66efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed datasets to:\n",
      "   C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\processed\\climate_train.csv\n",
      "   C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\processed\\climate_dev.csv\n",
      "   C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\processed\\climate_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Now lets save all CSV since we have successfully completed Processing data\n",
    "train_path = PROCESSED_DIR / \"climate_train.csv\"\n",
    "dev_path   = PROCESSED_DIR / \"climate_dev.csv\"\n",
    "test_path  = PROCESSED_DIR / \"climate_test.csv\"\n",
    "\n",
    "train_df_raw.to_csv(train_path, index=False)\n",
    "dev_df_raw.to_csv(dev_path, index=False)\n",
    "test_df_raw.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"Saved processed datasets to:\")\n",
    "print(\"  \", train_path)\n",
    "print(\"  \", dev_path)\n",
    "print(\"  \", test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7810c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (437, 7)\n",
      "dev shape: (115, 7)\n",
      "test shape: (133, 7)\n",
      "Saved combined CSVs to: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_checked_segment</th>\n",
       "      <th>comment_by_fact-checker</th>\n",
       "      <th>article</th>\n",
       "      <th>label_str_raw</th>\n",
       "      <th>label_str</th>\n",
       "      <th>label_id</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“climate economists see a positive externality...</td>\n",
       "      <td>This is cherry-picking at its worst. You can a...</td>\n",
       "      <td>article36.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "      <td>CHERRY_PICKING</td>\n",
       "      <td>0</td>\n",
       "      <td>climate economists see a positive externality,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The latest U.N. science compendium asserts tha...</td>\n",
       "      <td>The recent US National Climate Assessment1 fin...</td>\n",
       "      <td>article130.txt</td>\n",
       "      <td>Cherry Picking</td>\n",
       "      <td>CHERRY_PICKING</td>\n",
       "      <td>0</td>\n",
       "      <td>The latest U.N. science compendium asserts tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“‘If we are right, our study challenges decade...</td>\n",
       "      <td>It only potentially challenges ONE method used...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cherry Picking</td>\n",
       "      <td>CHERRY_PICKING</td>\n",
       "      <td>0</td>\n",
       "      <td>If we are right, our study challenges decades ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fact_checked_segment  \\\n",
       "0  “climate economists see a positive externality...   \n",
       "1  The latest U.N. science compendium asserts tha...   \n",
       "2  “‘If we are right, our study challenges decade...   \n",
       "\n",
       "                             comment_by_fact-checker         article  \\\n",
       "0  This is cherry-picking at its worst. You can a...   article36.txt   \n",
       "1  The recent US National Climate Assessment1 fin...  article130.txt   \n",
       "2  It only potentially challenges ONE method used...             NaN   \n",
       "\n",
       "    label_str_raw       label_str  label_id  \\\n",
       "0  Cherry Picking  CHERRY_PICKING         0   \n",
       "1  Cherry Picking  CHERRY_PICKING         0   \n",
       "2  Cherry Picking  CHERRY_PICKING         0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  climate economists see a positive externality,...  \n",
       "1  The latest U.N. science compendium asserts tha...  \n",
       "2  If we are right, our study challenges decades ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Load Preproceesed splits (for safety /re-runs) and build combined csvs ===\n",
    "# Assigning train, test or dev to saved csv's\n",
    "train_df = pd.read_csv(train_path)\n",
    "dev_df   = pd.read_csv(dev_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# Safety: enforce types\n",
    "for name, df in [(\"train\", train_df), (\"dev\", dev_df), (\"test\", test_df)]:\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].fillna(\"\").astype(str)\n",
    "    df[\"label_id\"]   = df[\"label_id\"].astype(int)\n",
    "    print(f\"{name} shape:\", df.shape)\n",
    "\n",
    "# Save individual combined CSVs\n",
    "train_df.to_csv(COMBINED_DIR / \"climate_train_combined.csv\", index=False)\n",
    "dev_df.to_csv(COMBINED_DIR / \"climate_dev_combined.csv\", index=False)\n",
    "test_df.to_csv(COMBINED_DIR / \"climate_test_combined.csv\", index=False)\n",
    "\n",
    "# Save full combined dataset (train+dev+test)\n",
    "full_df = pd.concat([train_df, dev_df, test_df], ignore_index=True)\n",
    "full_df.to_csv(COMBINED_DIR / \"climate_full_combined.csv\", index=False)\n",
    "\n",
    "print(\"Saved combined CSVs to:\", COMBINED_DIR)\n",
    "\n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc9f4533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution (full_train):\n",
      "label_id\n",
      "0      84\n",
      "1      39\n",
      "2      22\n",
      "3      40\n",
      "4      37\n",
      "5       6\n",
      "6     167\n",
      "7      12\n",
      "8      56\n",
      "9      29\n",
      "10     60\n",
      "Name: count, dtype: int64\n",
      "{0: 'CHERRY_PICKING', 1: 'EVADING_THE_BURDEN_OF_PROOF', 2: 'FALSE_ANALOGY', 3: 'FALSE_AUTHORITY', 4: 'FALSE_CAUSE', 5: 'HASTY_GENERALISATION', 6: 'NO_FALLACY', 7: 'POST_HOC', 8: 'RED_HERRINGS', 9: 'STRAWMAN', 10: 'VAGUENESS'}\n",
      "Original counts:\n",
      " label_id\n",
      "0      84\n",
      "1      39\n",
      "2      22\n",
      "3      40\n",
      "4      37\n",
      "5       6\n",
      "6     167\n",
      "7      12\n",
      "8      56\n",
      "9      29\n",
      "10     60\n",
      "Name: count, dtype: int64\n",
      "Max count: 167\n",
      "Balanced counts:\n",
      " label_id\n",
      "0     167\n",
      "1     167\n",
      "2     167\n",
      "3     167\n",
      "4     167\n",
      "5     167\n",
      "6     167\n",
      "7     167\n",
      "8     167\n",
      "9     167\n",
      "10    167\n",
      "Name: count, dtype: int64\n",
      "Saved balanced training data to: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\combined_csv\\climate_train_balanced.csv\n"
     ]
    }
   ],
   "source": [
    "# ===  Build full_train (train + dev), then UPSAMPLE ===\n",
    "\n",
    "full_train = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "\n",
    "print(\"Original label distribution (full_train):\")\n",
    "print(full_train[\"label_id\"].value_counts().sort_index())\n",
    "print({i: ID2LABEL[i] for i in sorted(ID2LABEL)})\n",
    "\n",
    "\n",
    "def balance_dataset(df: pd.DataFrame, label_col: str = \"label_id\") -> pd.DataFrame:\n",
    "    counts = df[label_col].value_counts()\n",
    "    max_count = counts.max()\n",
    "    print(\"Original counts:\\n\", counts.sort_index())\n",
    "    print(\"Max count:\", max_count)\n",
    "\n",
    "    balanced_parts = []\n",
    "    for label, count in counts.items():\n",
    "        subset = df[df[label_col] == label]\n",
    "        upsampled = subset.sample(\n",
    "            n=max_count,\n",
    "            replace=True,\n",
    "            random_state=42,\n",
    "        )\n",
    "        balanced_parts.append(upsampled)\n",
    "\n",
    "    balanced_df = (\n",
    "        pd.concat(balanced_parts)\n",
    "        .sample(frac=1, random_state=42)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(\"Balanced counts:\\n\", balanced_df[label_col].value_counts().sort_index())\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "full_train_balanced = balance_dataset(full_train, label_col=\"label_id\")\n",
    "\n",
    "# Save balanced training CSV\n",
    "balanced_path = COMBINED_DIR / \"climate_train_balanced.csv\"\n",
    "full_train_balanced.to_csv(balanced_path, index=False)\n",
    "print(\"Saved balanced training data to:\", balanced_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556a2896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Baseline on UNBALANCED full_train ===\n",
      "=== Baseline (TF-IDF + Linear SVM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.24      0.24        17\n",
      "           1       0.33      0.12      0.18         8\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.50      0.50      0.50         8\n",
      "           4       1.00      0.12      0.22         8\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.35      0.68      0.46        34\n",
      "           7       0.00      0.00      0.00         2\n",
      "           8       0.00      0.00      0.00        11\n",
      "           9       0.00      0.00      0.00         6\n",
      "          10       0.12      0.08      0.10        12\n",
      "\n",
      "    accuracy                           0.31       111\n",
      "   macro avg       0.23      0.16      0.15       111\n",
      "weighted avg       0.29      0.31      0.25       111\n",
      "\n",
      "\n",
      "=== Baseline on BALANCED full_train_balanced ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (TF-IDF + Linear SVM) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74        33\n",
      "           1       0.91      0.97      0.94        33\n",
      "           2       0.87      1.00      0.93        33\n",
      "           3       0.89      0.91      0.90        34\n",
      "           4       0.97      0.94      0.96        34\n",
      "           5       0.94      1.00      0.97        33\n",
      "           6       0.81      0.62      0.70        34\n",
      "           7       0.94      1.00      0.97        34\n",
      "           8       0.86      0.88      0.87        34\n",
      "           9       0.97      0.94      0.95        33\n",
      "          10       0.88      0.91      0.90        33\n",
      "\n",
      "    accuracy                           0.90       368\n",
      "   macro avg       0.89      0.90      0.89       368\n",
      "weighted avg       0.89      0.90      0.89       368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model (TF-IDF + Linear SVM)\n",
    "\n",
    "cfg = BaselineConfig(\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Baseline on UNBALANCED full_train ===\")\n",
    "baseline_model_unbal, baseline_vec_unbal = run_baseline_multiclass(\n",
    "    df=full_train,            # unbalanced\n",
    "    text_col=\"text_clean\",\n",
    "    label_col=\"label_id\",\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Baseline on BALANCED full_train_balanced ===\")\n",
    "baseline_model_bal, baseline_vec_bal = run_baseline_multiclass(\n",
    "    df=full_train_balanced,   # balanced\n",
    "    text_col=\"text_clean\",\n",
    "    label_col=\"label_id\",\n",
    "    cfg=cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b21d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d379585b1bd84403aeaf9907e2709e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05c3deea5344f25837f244953c8f3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3554bcad07f54984a03af680fef0c320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilroberta-base/resolve/main/vocab.json: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67065af33a1d47109b0fc1aeaa110f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faa3b1ad49343608c5c10621913a112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a9aabffce845babb77a704325f5795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12e78c203ab449b977ad8195f915184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb34b28997184ce688353927268a3bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (by label_id): {np.int64(0): np.float64(0.9966078697421981), np.int64(1): np.float64(0.9966078697421981), np.int64(2): np.float64(0.9966078697421981), np.int64(3): np.float64(1.0041011619958988), np.int64(4): np.float64(1.0041011619958988), np.int64(5): np.float64(0.9966078697421981), np.int64(6): np.float64(1.0041011619958988), np.int64(7): np.float64(1.0041011619958988), np.int64(8): np.float64(1.0041011619958988), np.int64(9): np.float64(0.9966078697421981), np.int64(10): np.float64(0.9966078697421981)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41b11150114d95b68fc9bf7ec7b7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\src\\models\\slm_finetune.py:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [920/920 1:40:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.358800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.280800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.223800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "slm_cfg = SLMConfigMC(\n",
    "    model_name=\"distilroberta-base\",\n",
    "    max_length=256,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=16,\n",
    "    num_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    output_dir=str(PROJECT_ROOT / \"outputs\" / \"slm_climate_multiclass\")\n",
    ")\n",
    "\n",
    "slm_trainer = train_slm_multiclass(\n",
    "    df=full_train_balanced,\n",
    "    text_col=\"text_clean\",\n",
    "    label_col=\"label_id\",\n",
    "    cfg=slm_cfg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c2062d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model + tokenizer to: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\outputs\\slm_climate_multiclass\n"
     ]
    }
   ],
   "source": [
    "# Saving model to \"slm_climate_multiclass folder in output\"\n",
    "# creating its dir\n",
    "model_dir = PROJECT_ROOT / \"outputs\" / \"slm_climate_multiclass\"\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model + tokenizer into  folder\n",
    "slm_trainer.save_model(model_dir)\n",
    "slm_trainer.tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "print(\"Saved model + tokenizer to:\", model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f384b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\outputs\\slm_climate_multiclass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81924b4676dd40729fd1774a72aab9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30840\\510423563.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_eval = Trainer(model=model, tokenizer=tokenizer)\n",
      "c:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "             CHERRY_PICKING       0.15      0.19      0.17        21\n",
      "EVADING_THE_BURDEN_OF_PROOF       0.40      0.22      0.29         9\n",
      "              FALSE_ANALOGY       0.38      0.60      0.46         5\n",
      "            FALSE_AUTHORITY       0.50      0.30      0.38        10\n",
      "                FALSE_CAUSE       0.25      0.22      0.24         9\n",
      "       HASTY_GENERALISATION       0.00      0.00      0.00         2\n",
      "                 NO_FALLACY       0.34      0.24      0.29        41\n",
      "                   POST_HOC       0.00      0.00      0.00         2\n",
      "               RED_HERRINGS       0.27      0.31      0.29        13\n",
      "                   STRAWMAN       0.11      0.14      0.12         7\n",
      "                  VAGUENESS       0.25      0.43      0.32        14\n",
      "\n",
      "                   accuracy                           0.26       133\n",
      "                  macro avg       0.24      0.24      0.23       133\n",
      "               weighted avg       0.28      0.26      0.26       133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading  from  fine-tuned model directory\n",
    "model_dir = PROJECT_ROOT / \"outputs\" / \"slm_climate_multiclass\"\n",
    "print(\"Loading model from:\", model_dir)\n",
    "\n",
    "# Load tokenizer + model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Tokenization function for evaluation\n",
    "def tokenize_fn_eval(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text_clean\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "# Prepare test dataset for HF Trainer\n",
    "test_ds = Dataset.from_pandas(\n",
    "    test_df[[\"text_clean\", \"label_id\"]].reset_index(drop=True)\n",
    ")\n",
    "test_ds = test_ds.map(tokenize_fn_eval, batched=True)\n",
    "test_ds = test_ds.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "# Safely remove unused columns\n",
    "cols_to_remove = [\n",
    "    c for c in [\"text_clean\", \"__index_level_0__\"] if c in test_ds.column_names\n",
    "]\n",
    "test_ds = test_ds.remove_columns(cols_to_remove)\n",
    "\n",
    "# Ensure PyTorch format\n",
    "test_ds.set_format(\"torch\")\n",
    "\n",
    "# New Trainer instance for evaluation\n",
    "trainer_eval = Trainer(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer_eval.predict(test_ds)\n",
    "\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Print classification report with fallacy labels\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=[ID2LABEL[i] for i in sorted(ID2LABEL)],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcb037be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: In the sugarcane region of El Salvador, as much as one-fifth of the population has chronic kidney disease, including over a quarter of the men, the presumed result of dehydration from working the fields they were able to comfortably harvest as recently as two decades ago.\n",
      "Predicted fallacy: CHERRY_PICKING\n",
      "Prediction confidence: 0.8579\n"
     ]
    }
   ],
   "source": [
    "sample_text = ( \"In the sugarcane region of El Salvador, as much as one-fifth of the population has chronic kidney disease, including over a quarter of the men, the presumed result of dehydration from working the fields they were able to comfortably harvest as recently as two decades ago.\"\n",
    ")\n",
    "\n",
    "# Token Input\n",
    "inputs = tokenizer(\n",
    "    sample_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "# Forward Pass\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Predicted Class ID\n",
    "pred_id = int(logits.argmax().item())\n",
    "pred_label = ID2LABEL[pred_id]\n",
    "\n",
    "# Softmax Probabilitied\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "pred_confidence = float(probs[0][pred_id].item())\n",
    "\n",
    "print(\"Sample text:\", sample_text)\n",
    "print(\"Predicted fallacy:\", ID2LABEL[pred_id])\n",
    "print(f\"Prediction confidence: {pred_confidence:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5acdfe79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f3b73424324b0a8c50bab748ff8780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ecac1ebbce4cfc815230f812a79691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4a5906152d415f931c620fa1c1dcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4485bec8b74d8692a5e8a4c4f3caab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13dfb3705e242c18c73c0e3cfea28d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a2660cd64b4d1d8ed6b6b08a0d4784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73f5cefb4ab4c74b3dab17f35f54309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explainer = QwenExplainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79d2cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Example index: 13\n",
      "- Text:\n",
      "The particular signature of warming in 2016 was also revealing in another way, Overpeck said, noting that the stratosphere saw record cold temperatures last year\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: CHERRY_PICKING\n",
      "\n",
      "Qwen explanation:\n",
      "Cherry picking occurs when someone selectively chooses only certain data points or evidence to support their claim while ignoring contradictory information. In this case, the author seems to have cherry picked the temperature data from the stratosphere to support their claim about global warming without considering other factors such as oceanic temperatures or land surface temperatures.\n",
      "This text could potentially be an example of cherry picking because it focuses on one specific piece of data (record cold temperatures in the stratosphere) to argue for global warming, but overlooks other important variables like oceanic temperatures and land surface temperatures which may provide a more comprehensive view of overall climate trends. The author's selective use of data suggests they are trying to make a narrow interpretation of global warming based solely on this isolated observation rather than considering broader climatic patterns. To avoid cherry picking\n",
      "\n",
      "================================================================================\n",
      "Example index: 14\n",
      "- Text:\n",
      "For example, Canadian polar bear biologist Ian Stirling learned in the 1970s that spring sea ice in the southern Beaufort Sea periodically gets so thick that seals depart, depriving local polar bears of their prey and causing their numbers to plummet. But that fact, documented in more than a dozen scientific papers, is not discussed today as part of polar bear ecology.\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: CHERRY_PICKING\n",
      "\n",
      "Qwen explanation:\n",
      "Cherry picking involves selectively choosing only parts of data or evidence that support one's point while ignoring other relevant information. In this case, the author selectively chooses to highlight only the negative impacts on polar bears due to lack of sea ice, but ignores the positive aspects such as increased research efforts and funding for polar bear conservation. This selective approach makes it appear as if there has been no progress in understanding polar bear ecology despite significant advancements in science. The cherry-picked example focuses solely on the decline in seal populations without mentioning any improvements in polar bear habitat preservation or management strategies. Therefore, this text could be considered an example of cherry picking because it presents a narrow view of the topic by omitting important context and balanced perspectives.\n",
      "Cherry picking occurs when someone selects only certain pieces of information from a larger set to\n",
      "\n",
      "================================================================================\n",
      "Example index: 19\n",
      "- Text:\n",
      "Until now, scientists have calculated the temperature of the ancient seas by looking at foraminifera, the fossils of tiny marine organisms found in the sediment on the ocean floor.\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: CHERRY_PICKING\n",
      "\n",
      "Qwen explanation:\n",
      "Cherry picking refers to selectively choosing only certain pieces of evidence or data while ignoring other relevant information. In this case, the person who made the post seems to be focusing only on one piece of evidence (the fossilized foraminifera) instead of considering all available data on the subject.\n",
      "This post could potentially be an example of cherry picking because it suggests that scientists have relied solely on one method (looking at foraminifera fossils) to calculate past temperatures without considering alternative methods or sources of evidence. However, it's important to note that the claim may not necessarily be true based on current scientific understanding. The full context would need to be considered to determine if cherry picking was indeed used here.\n",
      "\n",
      "================================================================================\n",
      "Example index: 0\n",
      "- Text:\n",
      "Whether Antarctic mass loss keeps worsening depends on choices made today\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: VAGUENESS\n",
      "\n",
      "Qwen explanation:\n",
      "In the given social media post, the author mentions \"Antarctic mass loss\" but does not specify whether it refers to ice sheets or glaciers. This vagueness makes it difficult for readers to understand exactly what the speaker is referring to. The fallacy of vagueness occurs when someone uses vague language that could lead to misunderstandings or confusion. Therefore, the text is likely an example of the fallacy of vagueness because it lacks clear specificity regarding which type of ice is being discussed. \n",
      "\n",
      "Note: While I have provided a detailed explanation, please remember that my goal here is to provide a concise response within the specified task requirements. If you need more information or clarification, feel free to ask!\n",
      "\n",
      "================================================================================\n",
      "Example index: 1\n",
      "- Text:\n",
      "This chart gives you a good idea of the direction of the adjustments.\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: NO_FALLACY\n",
      "\n",
      "Qwen explanation:\n",
      "The post does not contain any logical fallacy. It simply provides information about a chart showing adjustments to something. There's no attempt to mislead or manipulate the reader with false claims or unsupported statements. The statement \"This chart gives you a good idea\" is factual and informative rather than deceptive. \n",
      "\n",
      "No logical fallacy was identified in this case. The original text did not include any errors, contradictions, or misleading statements that would indicate a logical flaw. Therefore, it cannot be classified as an instance of a specific logical fallacy such as ad hominem, straw man, slippery slope, etc., since none were present. \n",
      "\n",
      "The provided explanation aims to clarify why the given prediction for a logical fallacy is incorrect by explaining what each type of fallacy entails and how they differ from the situation described\n",
      "\n",
      "================================================================================\n",
      "Example index: 2\n",
      "- Text:\n",
      "International Energy Agency, a global analysis organization, continue to see a role for coal for the foreseeable future.\n",
      "- True label:      CHERRY_PICKING\n",
      "- Predicted label: NO_FALLACY\n",
      "\n",
      "Qwen explanation:\n",
      "No fallacy detected. The statement does not contain any logical errors or assumptions that would make it susceptible to a particular type of logical fallacy.\n",
      "\n",
      "The post simply states a fact about the International Energy Agency's view on coal's continued relevance in the energy sector. It doesn't present any unsupported claims or arguments that could be subject to fallacious reasoning. Therefore, no specific logical fallacy can be identified here. \n",
      "\n",
      "This explanation clarifies why the given prediction of \"NO_FALLACY\" is correct based on the content provided. The key points are:\n",
      "\n",
      "- Understanding what the fallacy means.\n",
      "- Identifying whether there are any logical flaws in the statement.\n",
      "- Justification for concluding that no specific fallacy applies. To clarify further, let's break down each component of your question:\n",
      "\n",
      "### Explanation of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Qwen for explanations\n",
    "# Mix of correct and incorrect predictions\n",
    "\n",
    "# Indices of correctly and incorrectly classified examples\n",
    "\n",
    "correct_idx = np.where(y_true == y_pred)[0][:3]\n",
    "wrong_idx   = np.where(y_true != y_pred)[0][:3]\n",
    "\n",
    "indices = list(correct_idx) + list(wrong_idx)\n",
    "\n",
    "for idx in indices:\n",
    "    row = test_df.iloc[idx]\n",
    "    text = row[\"text_clean\"]\n",
    "    true_label_id = int(row[\"label_id\"])\n",
    "    pred_label_id = int(y_pred[idx])\n",
    "\n",
    "    true_label_name = ID2LABEL[true_label_id]\n",
    "    pred_label_name = ID2LABEL[pred_label_id]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Example index: {idx}\")\n",
    "    print(\"- Text:\")\n",
    "    print(text)\n",
    "    print(f\"- True label:      {true_label_name}\")\n",
    "    print(f\"- Predicted label: {pred_label_name}\")\n",
    "\n",
    "    # Qwen Explanation\n",
    "    explanation = explainer.explain(text, pred_label_name)\n",
    "    print(\"\\nQwen explanation:\")\n",
    "    print(explanation)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2e551e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved explanations to: C:\\Users\\HP\\UG - DATA SCIENCE\\NLP\\Steadx01_NLP\\data\\Qwen_exp\\climate_test_with_phi35_explanations.csv\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "num_samples = 20  # or len(test_df) if you’re brave lol!!!\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    row = test_df.iloc[idx]\n",
    "    text = row[\"text_clean\"]\n",
    "    true_label_id = int(row[\"label_id\"])\n",
    "    pred_label_id = int(y_pred[idx])\n",
    "\n",
    "    true_label_name = ID2LABEL[true_label_id]\n",
    "    pred_label_name = ID2LABEL[pred_label_id]\n",
    "\n",
    "    explanation = explainer.explain(text, pred_label_name)\n",
    "\n",
    "    records.append({\n",
    "        \"index\": idx,\n",
    "        \"text\": text,\n",
    "        \"true_label\": true_label_name,\n",
    "        \"pred_label\": pred_label_name,\n",
    "        \"qwen_explanation\": explanation,\n",
    "    })\n",
    "\n",
    "exp_df = pd.DataFrame(records)\n",
    "exp_path = PROJECT_ROOT / \"data\" / \"Qwen_exp\" / \"climate_test_with_phi35_explanations.csv\"\n",
    "exp_df.to_csv(exp_path, index=False)\n",
    "print(\"Saved explanations to:\", exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb67f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
